# ğŸ—£ï¸ Darija Speech Recognizer & Voice Assistant

A deep learning-powered speech recognition system specifically designed for **Moroccan Darija** (Moroccan Arabic dialect). This project uses advanced neural networks to recognize voice commands and can be deployed as a web application.

## ğŸ¯ Project Overview

The goal is to recognize short voice commands in Moroccan Darija like "ÙØªØ­ Ø§Ù„Ø¨Ø§Ø¨" (open the door), "Ø´Ø¹Ù„ Ø§Ù„Ø¶ÙˆØ¡" (turn on the light), etc., and test them via a simple command-line assistant.

This project includes:
- ğŸ§  A trained speech recognition model
- ğŸ’¬ A CLI assistant to test predictions  
- ğŸ§ Test audio samples
- ğŸ““ A Jupyter notebook for training
- âš™ï¸ Scripts to run and interact with the model

The system combines traditional audio signal processing with modern deep learning techniques to achieve accurate speech recognition for the Darija dialect.

---

## ğŸ§  Model Architecture & Technology Stack

### Deep Learning Models Used

#### 1. **Primary Model: Multi-Layer LSTM Network**
```
Input: MFCC Features (40 features Ã— 100 time steps)
    â†“
LSTM Layer 1: 128 units (return_sequences=True)
    â†“
Dropout: 0.3
    â†“
LSTM Layer 2: 64 units (return_sequences=True)  
    â†“
Dropout: 0.3
    â†“
LSTM Layer 3: 32 units
    â†“
Dropout: 0.3
    â†“
Dense Layer 1: 64 units (ReLU activation)
    â†“
Dropout: 0.5
    â†“
Dense Layer 2: 32 units (ReLU activation)
    â†“
Output Layer: N classes (Softmax activation)
```

**Why LSTM?**
- **Sequential Processing**: Speech is inherently sequential data
- **Long-term Dependencies**: Can remember patterns across time
- **Variable Length Handling**: Adapts to different speech durations
- **Temporal Relationships**: Captures phonetic transitions in Darija

#### 2. **Feature Extraction: MFCC (Mel-Frequency Cepstral Coefficients)**
- **40 MFCC coefficients** per frame
- **22,050 Hz sampling rate**
- **3-second audio duration** (padded/truncated)
- **100 time steps** maximum sequence length

### Technical Specifications

| Component | Details |
|-----------|---------|
| **Audio Processing** | LibROSA library for audio loading and MFCC extraction |
| **Deep Learning Framework** | TensorFlow/Keras 2.10+ |
| **Model Type** | Sequential LSTM with Dense layers |
| **Optimizer** | Adam optimizer |
| **Loss Function** | Sparse Categorical Crossentropy |
| **Regularization** | Dropout layers (0.3-0.5) |
| **Callbacks** | EarlyStopping, ReduceLROnPlateau, ModelCheckpoint |

---

## ğŸ—‚ï¸ Project Structure

```
darija_speech_recognizer/
â”œâ”€â”€ ğŸ““ darija_speech_recognizer.ipynb    # Main training notebook
â”œâ”€â”€ ğŸ¤– darija_voice_assistant.py         # CLI assistant for testing
â”œâ”€â”€ ğŸ“Š augmented_data.csv                # Dataset metadata
â”œâ”€â”€ ğŸµ audio/
â”‚   â”œâ”€â”€ augmented_dataset/               # Training audio files
â”‚   â”œâ”€â”€ 7allbab.wav                     # Test sample
â”‚   â”œâ”€â”€ cha3aldo.wav                    # Test sample
â”‚   â””â”€â”€ tfido.wav                       # Test sample
â”œâ”€â”€ ğŸ§  deployment_model/
â”‚   â”œâ”€â”€ darija_speech_model.h5          # Trained model
â”‚   â”œâ”€â”€ label_encoder.pkl               # Label encoder
â”‚   â”œâ”€â”€ model_config.json               # Model configuration
â”‚   â””â”€â”€ requirements.txt                # Dependencies
â”œâ”€â”€ ğŸ“ˆ plots/
â”‚   â”œâ”€â”€ training_history.png            # Training metrics
â”‚   â””â”€â”€ confusion_matrix.png            # Model performance
â”œâ”€â”€ ğŸ“‹ requirements.txt                  # Project dependencies
â””â”€â”€ ğŸ“– README.md                        # This file
```

---

## ğŸ™ï¸ Dataset & Data Processing

### Dataset Composition
- **Training Dataset**: Audio files stored in `audio/augmented_dataset/`
- **Metadata**: Stored in `augmented_data.csv` with intent labels and Darija phrases
- **Test Samples**: 3 example audio files:
  - `7allbab.wav` - Test audio sample
  - `cha3aldo.wav` - Test audio sample  
  - `tfido.wav` - Test audio sample
- **Audio Format**: WAV files, 22.05 kHz, mono channel
- **Duration**: 3 seconds (standardized)

### Data Augmentation Techniques
The project uses advanced audio augmentation to improve model robustness:

```python
# Augmentation techniques applied:
- Background noise injection
- Speed variation (0.8x - 1.2x)
- Volume adjustment (0.5x - 1.5x)  
- Pitch shifting (Â±2 semitones)
- Time stretching
- Echo/reverb effects
```

### Feature Engineering Pipeline
1. **Audio Loading**: LibROSA loads WAV files at 22.05 kHz
2. **Duration Normalization**: Pad/truncate to 3 seconds
3. **MFCC Extraction**: 40 coefficients per time frame
4. **Sequence Padding**: Standardize to 100 time steps
5. **Feature Normalization**: Scale features for neural network input

---

## ğŸ—ï¸ Model Training Process

### Training Configuration
```python
# Training hyperparameters
EPOCHS = 50
BATCH_SIZE = 32
VALIDATION_SPLIT = 0.2
LEARNING_RATE = Adam default (0.001)
EARLY_STOPPING_PATIENCE = 10
LR_REDUCTION_PATIENCE = 5
```

### Training Pipeline
1. **Data Loading**: Load CSV metadata and audio files
2. **Feature Extraction**: Convert audio to MFCC features
3. **Data Splitting**: 80% training, 20% testing (stratified)
4. **Model Compilation**: Configure LSTM architecture
5. **Training**: Fit model with callbacks for optimization
6. **Evaluation**: Generate metrics and visualizations
7. **Model Saving**: Export for deployment

### Performance Monitoring
- **Training/Validation Accuracy** tracking
- **Loss curves** visualization  
- **Confusion Matrix** for class-wise performance
- **Classification Report** with precision/recall/F1
- **Early Stopping** to prevent overfitting

---

## ğŸš€ Getting Started

### Prerequisites
```bash
Python 3.8+
TensorFlow 2.10+
LibROSA 0.9+
NumPy, Pandas, Scikit-learn
Matplotlib, Seaborn (for visualization)
```

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/CodeByIman/darija_speech_recognizer.git
cd darija_speech_recognizer
```

2. **Install dependencies**
```bash
pip install -r requirements.txt
```

3. **Train the model (optional)**
```bash
jupyter notebook darija_speech_recognizer.ipynb
```

4. **Test the assistant**
```bash
python darija_voice_assistant.py
```

---

## ğŸ§ª Usage Examples

### Command Line Testing
```bash
# Test with the provided sample audio files
python darija_voice_assistant.py

# The assistant will:
# 1. Load the trained model
# 2. Test the available audio samples (7allbab.wav, cha3aldo.wav, tfido.wav)
# 3. Display predictions with confidence scores for each sample
```

### Programmatic Usage
```python
from darija_speech_recognizer import DarijaSpeechRecognizer

# Load trained model
recognizer = DarijaSpeechRecognizer()
recognizer.load_trained_model('deployment_model')

# Predict single audio file
intent, confidence, probabilities = recognizer.predict_audio('test_audio.wav')
print(f"Predicted Intent: {intent}")
print(f"Confidence: {confidence:.3f}")
```

### Web App Integration (Streamlit)
```python
import streamlit as st
import tempfile

@st.cache_resource
def load_model():
    recognizer = DarijaSpeechRecognizer()
    recognizer.load_trained_model('deployment_model')
    return recognizer

def main():
    st.title("ğŸ¤ Darija Speech Recognition")
    recognizer = load_model()
    
    uploaded_file = st.file_uploader("Upload Audio", type=['wav', 'mp3'])
    if uploaded_file:
        # Process and predict...
```

---

## ğŸ“Š Model Performance

### Expected Performance Metrics
- **Training Accuracy**: 85-95%
- **Validation Accuracy**: 80-90%
- **Inference Time**: <1 second per audio file
- **Model Size**: ~2-5 MB

### Supported Darija Commands
The model recognizes voice commands based on the intents defined in your `augmented_data.csv` file. The exact commands and categories depend on your training data.

**Test Audio Files Available:**
- `7allbab.wav` - Test sample
- `cha3aldo.wav` - Test sample
- `tfido.wav` - Test sample

*Note: The specific intents and Darija phrases are loaded from your training dataset.*

---

## ğŸŒ Deployment Options

### 1. **Streamlit Web App**
```bash
streamlit run streamlit_app.py
```

### 2. **Flask API**
```python
from flask import Flask, request, jsonify

app = Flask(__name__)
recognizer = load_model()

@app.route('/predict', methods=['POST'])
def predict():
    # Handle audio upload and prediction
    pass
```

### 3. **Docker Deployment**
```dockerfile
FROM python:3.9-slim
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
CMD ["streamlit", "run", "app.py"]
```

---

## ğŸ”§ Advanced Features

### Model Customization
- **Hyperparameter Tuning**: Adjust LSTM units, dropout rates
- **Architecture Modifications**: Add/remove layers, change activation functions
- **Feature Engineering**: Experiment with different audio features (spectrograms, chromagrams)
- **Data Augmentation**: Custom augmentation strategies for Darija

### Performance Optimization
- **Model Quantization**: Reduce model size for mobile deployment
- **TensorRT Integration**: GPU acceleration for inference
- **Batch Prediction**: Process multiple audio files efficiently
- **Caching**: Cache extracted features for faster repeated predictions

---

## ğŸ“ˆ Future Enhancements

### Technical Improvements
- [ ] **Transformer Architecture**: Experiment with attention mechanisms
- [ ] **Real-time Processing**: Stream audio processing capabilities
- [ ] **Multi-language Support**: Extend to other Arabic dialects
- [ ] **Voice Activity Detection**: Automatic speech segment detection
- [ ] **Speaker Adaptation**: Personalized models for different speakers

### Application Features
- [ ] **Mobile App**: Flutter/React Native implementation
- [ ] **IoT Integration**: Smart home device control
- [ ] **Cloud API**: RESTful API service
- [ ] **Voice Feedback**: Text-to-speech responses in Darija
- [ ] **Conversation Context**: Multi-turn dialogue understanding

---

## ğŸ¤ Contributing

We welcome contributions! Here's how you can help:

1. **Fork** the repository
2. **Create** a feature branch (`git checkout -b feature/AmazingFeature`)
3. **Commit** your changes (`git commit -m 'Add AmazingFeature'`)
4. **Push** to the branch (`git push origin feature/AmazingFeature`)
5. **Open** a Pull Request

### Contribution Areas
- **Dataset Expansion**: Add more Darija voice samples
- **Model Improvements**: Experiment with new architectures
- **Web Interface**: Enhance the user interface
- **Documentation**: Improve code documentation
- **Testing**: Add unit tests and integration tests

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **TensorFlow/Keras** team for the deep learning framework
- **LibROSA** developers for audio processing tools
- **Streamlit** for the web app framework
- **Moroccan Arabic** linguistics community
- **Open source contributors** who make projects like this possible

---

## ğŸ“ Contact & Support

- **Author**: Iman
- **GitHub**: [@CodeByIman](https://github.com/CodeByIman)
- **Issues**: [GitHub Issues](https://github.com/CodeByIman/darija_speech_recognizer/issues)
- **Discussions**: [GitHub Discussions](https://github.com/CodeByIman/darija_speech_recognizer/discussions)

---

## ğŸ“š Technical References

### Research Papers
- "Deep Speech Recognition for Moroccan Darija"
- "LSTM Networks for Speech Recognition"
- "MFCC Feature Extraction for Audio Classification"

### Key Technologies
- **TensorFlow**: https://tensorflow.org
- **LibROSA**: https://librosa.org
- **Streamlit**: https://streamlit.io
- **Scikit-learn**: https://scikit-learn.org

---

<div align="center">

**â­ Star this repository if you found it helpful!**

**ğŸ”„ Fork it to contribute to Darija NLP research!**

</div>